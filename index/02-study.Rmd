# Methods
## Datasets
```{r datasets-small, out.width="100%"}
datasets_small <- read_csv("data/misc/datasets_small.csv", show_col_types = FALSE)
datasets_small |> 
  rename(`$N_P$` = Ppts,
         `$N_A$` = Admins) |> 
  knitr::kable(caption = "Descriptive statistics of datasets used. $N_P$: number of participants; $N_A$: number of administrations; Prop English: proportion of exposure to English.", 
               booktabs = T,
               escape = F) |> 
  kable_styling(font_size = 9,
                htmltable_class = 'lightable-minimal')
```
The data used in this study were vocabulary data collected using CDIs on bilingual populations. 
Specifically, we used data from Words & Sentences forms, which measures children's productive vocabularies. 
We used bilingual datasets from two origins: three datasets contributed to Wordbank [@frankWordbankOpenRepository2017; associated with @hoffDualLanguageExposure2012; @hoffWhatExplainsCorrelation2018; @legacyVocabularySizeSpeed2018; @marchmanLanguagespecificNatureGrammatical2004], and one dataset [associated with @mitchellCognatesAreAdvantaged2022] openly available on the Open Science Framework, with a total of `r datasets_small |> pull(Admins) |> sum() |> apa_num(format="d")` administrations from `r datasets_small |> pull(Ppts) |> sum() |> apa_num(format="d")` children. 
The details of the datasets are shown in Table \@ref(tab:datasets-small). 
We included all children and administrations that were marked as 'bilingual' by the original authors, and did not introduce any additional exclusion criteria.[^02-01]

[^02-01]: The datasets have two details of note.
First, they include children aged up to 49 months, which is beyond the age range that Words & Sentences CDI forms are designed for (viz. 18--36 months). 
We included these children as (1) bilingual children tend to have smaller vocabularies in each language than their monolingual counterparts, which suggests that the CDI forms remain a viable instrument even for older bilingual children, and (2) our analyses primarily focus on item-level information, which remain valid for older bilingual children.
Second, some children had low language exposures in one of their languages (<10%). 
We also included these children as we did not have a specific theoretical stance on language exposure requirements for a child to be counted as bilingual, and accepted the original authors' judgements that these children were still counted as 'bilingual'. 

## Predictors
Our list of predictors included both administration-level and word-level predictors. 
The administration-level information (age and language exposure) were included in the datasets, while the word-level information were retrieved either from CHILDES (frequency and MLU-w) or from previous psycholinguistic studies involving adult raters (concreteness).
We also included our key predictors of interest, TE knowledge, and phonological similarity, which were calculated directly from the data.
The data processing workflow was adapted from @braginskyConsistencyVariabilityChildren2019.

Datasets were retrieved from Wordbank [@frankWordbankOpenRepository2017] via the `wordbankr` package [@braginskyWordbankrAccessingWordbank2022], except for the data from @mitchellCognatesAreAdvantaged2022, which were processed directly from CSV files retrieved from the project's OSF page.
*Age* was defined in months.
Different datasets employed different instruments to measure *language exposure* values, including language exposure interviews [@hoffDualLanguageExposure2012; @hoffWhatExplainsCorrelation2018; @marchmanLanguagespecificNatureGrammatical2004], the Language Exposure Assessment Tool [@legacyVocabularySizeSpeed2018], and the Language Exposure Questionnaire using the Multilingual Approach to Parent Language Estimate [@mitchellCognatesAreAdvantaged2022]; we standardised these values as percentages.

Data from CHILDES were retrieved using childes-db [@sanchezChildesdbFlexibleReproducible2019] via the `childesr` package [@braginskyChildesrAccessingCHILDES2022]; data processing code was adapted from @braginskyConsistencyVariabilityChildren2019. 
For *frequency*, we derived unigram counts in CHILDES for the corresponding language. 
Counts were normalised to the length of the corpus, Laplace smoothed, and log transformed. 
For *MLU-w* for a given word, we calculated the mean length (in words) of all utterances containing the word, for all corpora of that language in CHILDES.
Estimates for words that occurred fewer than 10 times were treated as missing.

For *concreteness*, we used previously collected norms [@brysbaertConcretenessRatings402014], which were gathered by asking adults to rate word meanings on a 5-point scale from abstract to concrete.

*TE knowledge* was coded by mapping items in CDIs to unilemmas (cross-linguistic conceptual mappings, retrieved from Wordbank), verified by the judgement of native or advanced proficient speakers of each language.
If the child knew any word in the other language that mapped to the same unilemma, they were considered as knowing a TE. 
We excluded all words for which a TE was not present on the form for the other language. 

*Phonological similarity* was measured by first generating the IPA representations of each item using eSpeak NG [@vitolinsESpeakNG2022], followed by minor cleaning. 
We then calculated the the phonological similarity using the equation: 
$$
\textup{sim} = 1 - \frac{\textup{lev}(w_1, w_2)}{\textup{max}(\textup{len}(w_1), \textup{len}(w_2))}
$$
where 'sim' is phonological similarity, 'lev' is Levenshtein distance, 'max' is maximum, and 'len' is length in phonemes, as informed by @flocciaVocabularyYearOlds2018 and @fourtassiGrowthChildrenSemantic2020. 
In the cases where one word was mapped to multiple TEs, we took the maximum phonological similarity score. 

All numeric predictors were standardised (mean-centred and scaled), and TE knowledge was contrast coded as \{-0.5, 0.5\} for 'TE not known' and 'TE known' respectively. 

## Modelling
We constructed the Base Model using word production as the outcome variable, and all input (age, language exposure,[^02-02] frequency) and word predictors (concreteness, MLU-w) as additive fixed effects, including the interaction between frequency and language exposure (operationalised as `log(raw_freq * prop_exposure)`).[^02-03]
In the TE Model, we added TE knowledge as both an additive fixed effect, as well as interaction effects with all other predictors. 
Finally, in the Full Model, we further added phonological similarity as both an additive fixed effect, as well as interaction effects with all other predictors. 
For all models, we also fitted random intercepts for each child and each item. 
The model specifications are shown in Table \@ref(tab:models). 

[^02-02]: Language exposure refers to the proportion of exposure to the language of the item in question.
In other words, for an item in English, language exposure reflects the proportion of a child's language input that is in English.

[^02-03]: We considered a few possible ways to specify the interaction term between exposure and frequency.
One option was `log(raw_freq * prop_exposure)`, reflecting the expectation (due to the standard model) that information accumulation for a word is proportional to the amount of input received for that word, which relates to the product of exposure and frequency. 
However, this is mathematically equivalent to `log(raw_freq) + log(prop_exposure)`, leading to an overlap in contributions with the main effect of `log(raw_freq)`.
As such, we opted for the alternative of `log(raw_freq) * prop_exposure`, even though this specification is less theoretically meaningful; indeed, this model may reveal that input accumulation may not be linearly related to word knowledge.
We also considered a model that included only the interaction term and no main effects of frequency or language exposure; however, we had no _a priori_ reason to suspect that frequency and language exposure could not also affect word difficulties---conversely, for example, greater language exposure in one language may result in lower global surprisal for all words in that language, rendering them easier to learn in general.
Hence, we retained both main and interaction effect in our models.

```{r models, out.width="100%"}
models <- read_csv("data/misc/models.csv", show_col_types = FALSE)
models |> 
  knitr::kable(caption = "List of models used in Study 3.", 
               booktabs = T,
               escape = F) |> 
  kable_styling(font_size = 9,
                htmltable_class = 'lightable-minimal') |> 
  column_spec(2, width = "9cm")
```

We fitted these models using a Bayesian logistic modelling approach, using the `brms` package [@burknerBrmsPackageBayesian2017] in R, with `cmdstanr` [@gabryCmdstanrInterfaceCmdStan2022] as the backend and `bayestestR` [@makowskiBayestestRDescribingEffects2019] for posterior descriptions.
The Bayesian approach allowed the relatively complex models to converge (considering the large number of random intercepts fitted), and also provided informative model comparison metrics (namely, Bayes factors) for interpretation.
We also used horseshoe priors ($df = 3$) as a regularisation technique to shrink small coefficients towards zero while preserving large coefficients, which was particularly important for the more complex models that may have had many insignificant interaction terms [@carvalhoHandlingSparsityHorseshoe2009; see also @piironenHyperpriorChoiceGlobal2017; @piironenSparsityInformationRegularization2017].
Due to computational limitations, we fitted these models separately for each dataset. 

# Results
To illustrate the kind of trajectories we predict in our analyses, we plot example word learning trajectories for "dog" and "jump" from one dataset in Figure \@ref(fig:mr-demo), collapsed across children binned by language exposure.
These trajectories reflect how the probability that a child knows a word varies with age, along with other predictors such as language exposure, demonstrating input-level variability.
The difference between the trajectories for "dog", which is learnt earlier, and "jump", which is learnt later, reflects variability across items.

(ref:mr-demo-cap) Example trajectories for "dog" and "jump" derived from the Base Model fitted to the English--Spanish dataset from Marchman. Panel columns reflect proportion of language exposure in the corresponding language (i.e., exposure proportion of English for the first row, and exposure proportion of Spanish for the second row).
```{r mr-demo, out.width="100%", fig.cap="(ref:mr-demo-cap)"}
mr_complete <- readRDS("outputs/mr_complete.rds")
mr_model_base <- readRDS("models/modelling/mr_model_base.rds")
demo_lemmas <- c("dog", "jump")

get_ul_preds <- function(data, model, unilemmas) {
  ul_data <- data |> 
    filter(uni_lemma %in% unilemmas) 
  
  if (ul_data |> nrow() == 0) {
    stop(glue("Unilemmas not present in data"))
  }
  
  pred_vals <- ul_data |> 
    select(freq, concreteness, mlu, 
           freq_t, concreteness_t, mlu_t, 
           language, lang_item_id, item_definition, uni_lemma) |> 
    distinct()
  
  new_data <- expand_grid(pred_vals,
                          age = 18:35,
                          lang_prop = c(25, 50, 75),
                          te_known = as.factor(c(0, 1))) |> 
    mutate(age_t = retransform(age, data$age),
           lang_prop_t = retransform(lang_prop, data$lang_prop),
           type = "predicted") |> 
    get_log_pf()
  
  predicted <- predict(model, 
                       newdata = new_data, 
                       re_formula = ~(1|lang_item_id)) |> 
    as_tibble() |> 
    select(Estimate) |> 
    cbind(new_data)
  
  raw_data <- data |>
    filter(uni_lemma %in% unilemmas) |>
    mutate(Estimate = as.numeric(value) - 1) |>
    select(age, freq, lang_prop, concreteness, mlu, 
           age_t, freq_t, lang_prop_t, concreteness_t, mlu_t, 
           te_known, language, lang_item_id, item_definition, uni_lemma,
           Estimate) |>
    mutate(type = "raw")
  
  bind_rows(raw_data, predicted) |> 
    mutate(freq_t_binned = cut(freq_t, c(-Inf,-0.5,0,0.5,Inf)),
           lang_prop_binned = cut(lang_prop, c(0, 40, 60, 100),
                                  include.lowest = TRUE),
           type = as.factor(type))
}

mr_demo <- get_ul_preds(mr_complete, mr_model_base, demo_lemmas)
mr_ex <- mr_demo |> filter(type == "predicted")

ggplot(data = mr_ex, mapping = aes(col = uni_lemma)) +
  theme() +
  geom_smooth(formula = y ~ x,
              mapping = aes(x = age, y = Estimate), # , lty = te_known),
              method = "glm", method.args = list(family = "quasibinomial"), 
              se = FALSE) +
  # scale_linetype_discrete(name = "TE knowledge", 
  #                         labels = c("TE not known", "TE known")) +
  scale_color_manual(name = "Unilemma",
                     values = c("#24639e", "#cf4563")) +
  facet_grid(language ~ lang_prop_binned) +
  labs(x = "Age", y = "Value") +
  ylim(0, 1) +
  scale_x_continuous(sec.axis = sec_axis(~ . , 
                                         name = "Language exposure", 
                                         breaks = NULL, 
                                         labels = NULL))
```

Some of the variability between words was due to the contributions of various word-related predictors, which we can visualise by plotting trajectories at different levels of the predictor. 
For example, the effect of frequency on word knowledge is shown in Figure \@ref(fig:mr-pred). which demonstrates that word knowledge increases with age, with language exposure, and with word frequency.
In the remainder of the analyses, we investigate the coefficients of such predictors to determine the extent to which they affect early word learning in bilinguals.

(ref:mr-pred-cap) Effect of word frequency on word production, derived from the Base Model fitted on the English--Spanish dataset from Marchman. Panel columns reflect proportion of language exposure in the corresponding language (i.e., exposure proportion of English for the first row, and exposure proportion of Spanish for the second row).
```{r mr-pred, out.width="100%", fig.cap="(ref:mr-pred-cap)"}
mr_complete_pred <- readRDS("outputs/mr_complete_pred.rds")
ggplot() +
  theme() +
  geom_smooth(data = mr_complete_pred,
              formula = y ~ x,
              aes(x = age, y = Estimate, col = freq_t_binned), #, lty = te_known),
              method = "glm", method.args = list(family = "quasibinomial"), se = FALSE) +
  scale_color_discrete_sequential(name = "Std freq", palette = "BluGrn") +
  facet_grid(language ~ lang_prop_binned) +
  labs(x = "Age", y = "Value") +
  ylim(0, 1) +
  scale_x_continuous(sec.axis = sec_axis(~ . , 
                                         name = "Language exposure", 
                                         breaks = NULL, 
                                         labels = NULL))
```

## Base Model
```{r data-base}
mp_b <- readRDS("outputs/mp_b.rds")
```
Figure \@ref(fig:coef-base) shows the coefficient estimates for each predictor in the Base Model across all datasets. 
We found strong effects of most predictors in the expected directions: age ($\bar\beta =$ `r mp_b |> filter(Parameter == "b_age_t") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_b |> filter(Parameter == "b_age_t") |> pull(CI_low) |> mean()`, `r mp_b |> filter(Parameter == "b_age_t") |> pull(CI_high) |> mean()`]), frequency ($\bar\beta =$ `r mp_b |> filter(Parameter == "b_freq_t") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_b |> filter(Parameter == "b_freq_t") |> pull(CI_low) |> mean()`, `r mp_b |> filter(Parameter == "b_freq_t") |> pull(CI_high) |> mean()`]), proportion of exposure to the language ($\bar\beta =$ `r mp_b |> filter(Parameter == "b_lang_prop_t") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_b |> filter(Parameter == "b_lang_prop_t") |> pull(CI_low) |> mean()`, `r mp_b |> filter(Parameter == "b_lang_prop_t") |> pull(CI_high) |> mean()`]), concreteness ($\bar\beta =$ `r mp_b |> filter(Parameter == "b_concreteness_t") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_b |> filter(Parameter == "b_concreteness_t") |> pull(CI_low) |> mean()`, `r mp_b |> filter(Parameter == "b_concreteness_t") |> pull(CI_high) |> mean()`]), and MLU-w ($\bar\beta =$ `r mp_b |> filter(Parameter == "b_mlu_t") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_b |> filter(Parameter == "b_mlu_t") |> pull(CI_low) |> mean()`, `r mp_b |> filter(Parameter == "b_mlu_t") |> pull(CI_high) |> mean()`]).[^02-04]
Interestingly, there was no reliable interaction effect between frequency and proportion exposure ($\bar\beta =$ `r mp_b |> filter(Parameter == "b_freq_t:lang_prop_t") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_b |> filter(Parameter == "b_freq_t:lang_prop_t") |> pull(CI_low) |> mean()`, `r mp_b |> filter(Parameter == "b_freq_t:lang_prop_t") |> pull(CI_high) |> mean()`]). 

[^02-04]: Estimates are means of maximum a posteriori probability estimates. Credible intervals were derived by taking the mean of the extents of the highest density intervals, and are approximate.

(ref:coef-base-cap) Estimates of predictor coefficients for the Base Model fit on each dataset. Each point represents the coefficient for one dataset, and the bar displays the mean across all datasets. Filled points indicate coefficients with a Bayes factor of > 1000. The shaded area represents the region of practical equivalence to the null.
```{r coef-base, out.width='2.25in', out.height='2.5in', fig.width=3.37, fig.height=3.75, fig.align='center', fig.cap="(ref:coef-base-cap)"}
plot_coefs(mp_b)
```

## TE Model
```{r data-te}
mp_t <- readRDS("outputs/mp_t.rds")
```

Figure \@ref(fig:coef-te) shows the coefficient estimates for each predictor in the TE Model across all datasets. 
The main effects were largely similar to those of the Base Model, so we focus on the effects related to TEs.
First, there was a main effect of TE knowledge ($\bar\beta =$ `r mp_t |> filter(Parameter == "b_te_known1") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_t |> filter(Parameter == "b_te_known1") |> pull(CI_low) |> mean()`, `r mp_t |> filter(Parameter == "b_te_known1") |> pull(CI_high) |> mean()`]), although there was relatively large variability, with one model having posteriors that was >\ 95% within the region of practical equivalence.
There was also a negative TE interaction effect with age ($\bar\beta =$ `r mp_t |> filter(Parameter == "b_age_t:te_known1") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_t |> filter(Parameter == "b_age_t:te_known1") |> pull(CI_low) |> mean()`, `r mp_t |> filter(Parameter == "b_age_t:te_known1") |> pull(CI_high) |> mean()`]) and with frequency ($\bar\beta =$ `r mp_t |> filter(Parameter == "b_freq_t:te_known1") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_t |> filter(Parameter == "b_freq_t:te_known1") |> pull(CI_low) |> mean()`, `r mp_t |> filter(Parameter == "b_freq_t:te_known1") |> pull(CI_high) |> mean()`]); in other words, TE knowledge provided a greater boost to younger children and for less frequent words. 
All other interaction effects had posterior distributions that largely fell within the region of practical equivalence.

(ref:coef-te-cap) Estimates of predictor coefficients (main effects and interactions with TE) for the TE Model fit on each dataset. Each point represents the coefficient for one dataset, and the bar displays the mean across all datasets. Filled points indicate coefficients with a Bayes factor of > 1000. The shaded area represents the region of practical equivalence to the null.
```{r coef-te, out.width='3.75in', out.height='2.5in', fig.width=5.625, fig.height=3.75, fig.align='center', fig.cap="(ref:coef-te-cap)"}
plot_coefs(mp_t)
```

## Full Model
```{r data-full}
mp_f <- readRDS("outputs/mp_f.rds")
```

Figure \@ref(fig:coef-full) shows the coefficient estimates for each predictor in the Full Model across all datasets. 
The main and TE interaction effects were largely similar to those of the TE Model, although the frequency-by-TE interaction effect was slightly attenuated ($\bar\beta =$ `r mp_f |> filter(Parameter == "b_freq_t:te_known1") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_f |> filter(Parameter == "b_freq_t:te_known1") |> pull(CI_low) |> mean()`, `r mp_f |> filter(Parameter == "b_freq_t:te_known1") |> pull(CI_high) |> mean()`]).
There was also an additional interaction effect between TE knowledge and phonological similarity  ($\bar\beta =$ `r mp_f |> filter(Parameter == "b_te_known1:overlap_t") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_f |> filter(Parameter == "b_te_known1:overlap_t") |> pull(CI_low) |> mean()`, `r mp_f |> filter(Parameter == "b_te_known1:overlap_t") |> pull(CI_high) |> mean()`]), such that TE knowledge provided a greater boost when the TEs have greater phonological similarity, although one model had posteriors that was >\ 95% within the region of practical equivalence.
All other interaction effects had posterior distributions that largely fell within the region of practical equivalence.

(ref:coef-full-cap) Estimates of predictor coefficients (main effects, and interactions with phonological similarity and/or TE) for the Full Model fit on each dataset. Each point represents the coefficient for one dataset, and the bar displays the mean across all datasets. Filled points indicate coefficients with a Bayes factor of > 1000. The shaded area represents the region of practical equivalence to the null.
```{r coef-full, out.width='6in', out.height='2.5in', fig.width=9, fig.height=3.75, fig.align='center', fig.cap="(ref:coef-full-cap)"}
plot_coefs(mp_f)
```

```{r cor-full-data}
mp_f_mat <- mp_f |> select(Parameter, MAP, Dataset) |> 
  pivot_wider(names_from = Dataset, values_from = MAP) |> 
  column_to_rownames(var = "Parameter") |> 
  as.matrix() |> 
  cor()
```

### Consistency.
We additionally investigated the consistency in the Full Model predictor magnitude across the datasets to determine the extent to which different language combinations and data origins share similar factors affecting word knowledge. 
The correlogram is shown in Figure \@ref(fig:cor-full).
The mean pairwise correlation was `r (sum(mp_f_mat) - 4) / 12`, comparable to the production means found by Braginsky et al. (2019) for production data, supporting Hypothesis 5. 

(ref:cor-full-cap) Correlogram of predictor magnitudes for the Full Model across all datasets.
```{r cor-full, out.width="100%", fig.cap="(ref:cor-full-cap)"}
corrplot(mp_f_mat, 
         method = "color", 
         addCoef.col = 'white', 
         tl.col = "black", 
         tl.srt = 45, 
         tl.cex = .8, 
         tl.offset = 1,
         cl.ratio = .25, 
         number.cex = .8, 
         mar = rep(1,4),
         family = .font)
```

### Lexical categories
```{r data-llc}
mp_lc_f <- readRDS("outputs/mp_lc_f.rds")
```

Previous research has suggested that different lexical categories may be sensitive to different predictors [@braginskyConsistencyVariabilityChildren2019; @goodmanDoesFrequencyCount2008]. 
As such, we subdivided each dataset into lexical categories (nouns, verbs, adjectives, function words, and others) and reran the Full Model on each subset,[^02-05] dropping the "others" category due to its internal heterogeneity.

[^02-05]: This approach had better interpretability relative to the alternative, which would be to include lexical category as a moderator variable, thereby greatly increasing the number of interaction terms.

Figure \@ref(fig:coef-lc) shows the coefficient estimates for each predictor in the Full Model across all datasets, subsetted by lexical category. 
These results confirmed previous findings that frequency was a more important predictor for nouns ($\bar\beta =$ `r mp_lc_f |> filter(Parameter == "b_freq_t", lexical_class == "nouns") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_lc_f |> filter(Parameter == "b_freq_t", lexical_class == "nouns") |> pull(CI_low) |> mean()`, `r mp_lc_f |> filter(Parameter == "b_freq_t", lexical_class == "nouns") |> pull(CI_high) |> mean()`]) than for function words ($\bar\beta =$ `r mp_lc_f |> filter(Parameter == "b_freq_t", lexical_class == "function_words") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_lc_f |> filter(Parameter == "b_freq_t", lexical_class == "function_words") |> pull(CI_low) |> mean()`, `r mp_lc_f |> filter(Parameter == "b_freq_t", lexical_class == "function_words") |> pull(CI_high) |> mean()`]). 
Broadly, most of the other predictors seemed to be relatively consistent among lexical categories, with three notable exceptions.
First, the age-by-TE interaction was stronger for verbs ($\bar\beta =$ `r mp_lc_f |> filter(Parameter == "b_age_t:te_known1", lexical_class == "verbs") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_lc_f |> filter(Parameter == "b_age_t:te_known1", lexical_class == "verbs") |> pull(CI_low) |> mean()`, `r mp_lc_f |> filter(Parameter == "b_age_t:te_known1", lexical_class == "verbs") |> pull(CI_high) |> mean()`]) than for other lexical categories, suggesting a greater TE boost for verbs in younger children.
Second, the frequency-by-TE interaction was positive for adjectives ($\bar\beta =$ `r mp_lc_f |> filter(Parameter == "b_freq_t:te_known1", lexical_class == "adjectives") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_lc_f |> filter(Parameter == "b_freq_t:te_known1", lexical_class == "adjectives") |> pull(CI_low) |> mean()`, `r mp_lc_f |> filter(Parameter == "b_freq_t:te_known1", lexical_class == "adjectives") |> pull(CI_high) |> mean()`]) whereas it was negative for other lexical categories; this may have been driven by particular semantic categories (e.g., colour words), which both have higher frequency and are commonly learnt by children in both languages.
Third, the phonological similarity-by-TE interaction was positive for nouns ($\bar\beta =$ `r mp_lc_f |> filter(Parameter == "b_te_known1:overlap_t", lexical_class == "nouns") |> pull(Estimate) |> mean()`, 89\% CI [`r mp_lc_f |> filter(Parameter == "b_te_known1:overlap_t", lexical_class == "nouns") |> pull(CI_low) |> mean()`, `r mp_lc_f |> filter(Parameter == "b_te_known1:overlap_t", lexical_class == "nouns") |> pull(CI_high) |> mean()`]) whereas it was equivalent to zero for other lexical categories, suggesting that phonological similarity between TE pairs was particularly important for nouns, supporting the findings from @schelletterBilingualChildrenLexical2005.
These results indicate that some predictors differ in importance across lexical categories, providing support for Hypothesis 6.

(ref:coef-lc-cap) Estimates of predictor coefficients (main effects, and interactions with phonological similarity and/or TE) for the Full Model fit on each dataset, subsetted by lexical category. Each point represents the coefficient for one dataset, and the bar displays the mean across all datasets. Filled points indicate coefficients with a Bayes factor of > 1000. The shaded area represents the region of practical equivalence to the null.
```{r coef-lc, out.width='6in', out.height='2.7in', fig.width=9, fig.height=4, fig.align='center', fig.cap="(ref:coef-lc-cap)"}
ggplot(mp_lc_f, aes(x = Estimate, 
                    y = fct_rev(fct_inorder(Predictor)),
                    col = lexical_class)) +
  facet_grid(. ~ Effect, scales = "free") +
  geom_rect(aes(xmin = ROPE_low[1], xmax = ROPE_high[1],
                  ymin = -Inf, ymax = Inf),
              fill = "gray90", alpha = .1, color = NA) +
  geom_vline(xintercept = 0, color = "gray30", linetype = "dashed") +
  geom_point(aes(shape = log_BF <= 3), size = 1, alpha = 1) +
  stat_summary(geom = "crossbar", 
               fun = mean, fun.min = mean, fun.max = mean,
               width = 0.5, fatten = 2) +
  scale_colour_discrete(name = "Lexical category",
                        labels = c("Adjectives", "Function words", "Nouns", "Verbs")) +
  scale_shape_manual(values = c(19, 1), guide = "none") +
  labs(x = "Coefficient estimate", y = "") +
  theme(legend.position = "bottom")
```
