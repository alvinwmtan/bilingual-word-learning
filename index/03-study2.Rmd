# Study 2: Modelling word production {#word-prod}
In this study, we modelled bilingual children's productive vocabulary from CDI datasets, aiming to understand the standard model as a model of word learning in bilingual contexts, and to investigate the contribution of TEs to word learning.
Under the standard model [@kachergisStandardModelEarly2022], learning can be conceived of as an accumulation of information from language input about a given word, such that the child knows the word once a threshold is reached, which may differ for different words, relating to the difficulty of the word. 
As such, we constructed our model by including input-related factors, including the administration-level variables examined in the previous sections (age and language exposure) as well as a distributional property of words (frequency). 
We also include factors relating to item difficulty, including semantic and syntactic variables previously demonstrated to influence word learning [concreteness and mean length of utterance in words (MLU-w); see @braginskyConsistencyVariabilityChildren2019]. 
With the standard model as a baseline, we added TEs and phonological similarity as additional predictive variables to determine if they explained some of the variance in word knowledge.
We then examined the contributions of these variables, specifically testing the hypotheses that older children have better word knowledge, that greater frequency and language exposure predict better word knowledge, as does the interaction between them, that higher concreteness and lower MLU-w predict better word knowledge, that knowing a word predicts knowing its corresponding TE, and that greater phonological similarity predicts better word knowledge only when the word's TE is known.

## Datasets
```{r datasets-small, out.width="100%"}
datasets_small <- read_csv("data/misc/datasets_small.csv", show_col_types = FALSE)
datasets_small |> 
  rename(`$N_P$` = Ppts,
         `$N_A$` = Admins) |> 
  knitr::kable(caption = "Descriptive statistics of datasets used in Study 2. $N_P$: number of participants; $N_A$: number of administrations.", 
               booktabs = T,
               escape = F) |> 
  kable_styling(font_size = 9,
                htmltable_class = 'lightable-minimal')
```
The data used in this study were vocabulary data collected using CDIs on bilingual populations. 
Specifically, we used data from Words & Sentences forms, which measures children's productive vocabularies. 
We used bilingual datasets from two origins: three datasets contributed to Wordbank [@frankWordbankOpenRepository2017; associated with @hoffDualLanguageExposure2012; @hoffWhatExplainsCorrelation2018; @legacyVocabularySizeSpeed2018; @marchmanLanguagespecificNatureGrammatical2004], and one dataset [associated with @mitchellCognatesAreAdvantaged2022] openly available on the Open Science Framework, with a total of `r datasets_small |> pull(Admins) |> sum() |> apa_num(format="d")` administrations from `r datasets_small |> pull(Ppts) |> sum() |> apa_num(format="d")` children. 
These were the available datasets for which age, language exposure, and item-level information were included, and for which related frequency information was available. 
The details of the datasets are shown in Table \@ref(tab:datasets-small). 

## Methods
### Predictors
Our list of predictors included both administration-level and word-level predictors. 
The administration-level information (age and language exposure) were included in the datasets, while the word-level information were retrieved either from CHILDES (frequency and MLU-w) or from previous psycholinguistic studies involving adult raters (concreteness).
The data processing workflow was adapted from @braginskyConsistencyVariabilityChildren2019.

Data from CHILDES were retrieved using childes-db [@sanchezChildesdbFlexibleReproducible2019] via the `childesr` package [@braginskyChildesrAccessingCHILDES2022]; data processing code was adapted from @braginskyConsistencyVariabilityChildren2019. 
For frequency, we derived unigram counts in CHILDES for the corresponding language. 
Counts were normalised to the length of the corpus, Laplace smoothed, and log transformed. 
For MLU-w for a given word, we calculated the mean length (in words) of all utterances containing the word, for all corpora of that language in CHILDES.
Estimates for words that occurred fewer than 10 times were treated as missing.

For concreteness, we used previously collected norms [@brysbaertConcretenessRatings402014], which were gathered by asking adults to rate word meanings on a 5-point scale from abstract to concrete.

We had two additional predictors used in our more complex models: TE knowledge and phonological similarity.
TE knowledge was coded by mapping items in CDIs to unilemmas (cross-linguistic conceptual mappings), verified by the judgement of native or advanced proficient speakers of each language.
If the child knew any word in the other language that mapped to the same unilemma, they were considered as knowing a TE. 
We excluded all words for which a TE was not present on the form for the other language. 

Phonological similarity was measured by first generating the IPA representations of each item using eSpeak NG [@vitolinsESpeakNG2022], followed by minor cleaning. 
We then calculated the the phonological similarity using the equation: 
$$
\textup{sim} = 1 - \frac{\textup{lev}(w_1, w_2)}{\textup{max}(\textup{len}(w_1), \textup{len}(w_2))}
$$
where 'sim' is phonological similarity, 'lev' is Levenshtein distance, 'max' is maximum, and 'len' is length in phonemes, as informed by @flocciaVocabularyYearOlds2018. 
In the cases where one word was mapped to multiple TEs, we took the maximum phonological similarity score. 

All numeric predictors were standardised (mean-centred and scaled), and TE knowledge was contrast coded as \{-0.5, 0.5\} for 'TE not known' and 'TE known' respectively. 

### Modelling
We constructed Model 1 as a baseline using word production as the outcome variable, and all input (age, language exposure,[^04-01] frequency) and word predictors (concreteness, MLU-w) as additive fixed effects, including the interaction between frequency and language exposure (operationalised as `log(raw_freq * prop_exposure)`.[^04-02]
In Model 2, we added TE knowledge as both an additive fixed effect, as well as interaction effects with all other predictors. 
Finally, in Model 3, we further added phonological similarity as both an additive fixed effect, as well as interaction effects with all other predictors. 
For all models, we also fitted random intercepts for each child and each item. 
The full model specifications are shown in Table \@ref(tab:models). 

[^04-01]: Language exposure refers to the proportion of exposure to the language of the item in question.
In other words, for an item in English, language exposure reflects the proportion of a child's language input that is in English.

[^04-02]: According to the standard model, we expect information accumulation for a word to be proportional to the amount of input received for that word, which relates to the product of exposure and frequency. 
As such, we regarded `log(raw_freq * prop_exposure)` to be more theoretically meaningful than the alternative method for specifying the interaction, namely `log(raw_freq) * prop_exposure`, even if this meant that the contributions of language exposure in the main and interaction effects were no longer along the same scale.
We also considered a model that included only the interaction term and no main effects of frequency or language exposure; however, we had no _a priori_ reason to suspect that frequency and language exposure could not also affect word difficulties---conversely, for example, greater language exposure in one language may result in lower global surprisal for all words in that language, rendering them easier to learn in general.
Hence, we retained both main and interaction effect in our models.

```{r models, out.width="100%"}
models <- read_csv("data/misc/models.csv", show_col_types = FALSE)
models |> 
  knitr::kable(caption = "List of models used in Study 3.", 
               booktabs = T,
               escape = F) |> 
  kable_styling(font_size = 9,
                htmltable_class = 'lightable-minimal') |> 
  column_spec(2, width = "9cm")
```

We fitted these models using a Bayesian modelling approach, using the `brms` package [@burknerBrmsPackageBayesian2017] in R, with `cmdstanr` [@gabryCmdstanrInterfaceCmdStan2022] as the backend and `bayestestR` [@makowskiBayestestRDescribingEffects2019] for posterior descriptions.
The Bayesian approach allowed the relatively complex models to converge (considering the large number of random intercepts fitted), and also provided informative model comparison metrics (namely, Bayes factors) for interpretation.
We also used horseshoe priors ($df = 3$) as a regularisation technique to shrink small coefficients towards zero while preserving large coefficients, which was particularly important for the more complex models that may have had many insignificant interaction terms [@carvalhoHandlingSparsityHorseshoe2009; see also @piironenHyperpriorChoiceGlobal2017; @piironenSparsityInformationRegularization2017].
Due to computational limitations, we fitted these models separately for each dataset. 

## Results
To illustrate our analytic methodology, we first report results from one model, specifically Model 1 fitted on the English--Spanish dataset from Marchman. 
The fitted models effectively provided word learning trajectories for each child and each item. 
Example trajectories for unilemmas "dog" and "jump" are shown in Figure \@ref(fig:mr-demo), collapsed across children binned by language exposure.

(ref:mr-demo-cap) Example trajectories for "dog" and "jump" derived from Model 1 fitted on the English--Spanish dataset from Marchman.
```{r mr-demo, out.width="100%", fig.cap="(ref:mr-demo-cap)"}
mr_complete <- readRDS("outputs/mr_complete.rds")
mr_model_base <- readRDS("models/modelling/mr_model_base.rds")
demo_lemmas <- c("dog", "jump")

get_ul_preds <- function(data, model, unilemmas) {
  ul_data <- data |> 
    filter(uni_lemma %in% unilemmas) 
  
  if (ul_data |> nrow() == 0) {
    stop(glue("Unilemmas not present in data"))
  }
  
  pred_vals <- ul_data |> 
    select(freq, concreteness, mlu, 
           freq_t, concreteness_t, mlu_t, 
           language, lang_item_id, item_definition, uni_lemma) |> 
    distinct()
  
  new_data <- expand_grid(pred_vals,
                          age = 18:35,
                          lang_prop = c(25, 50, 75),
                          te_known = as.factor(c(0, 1))) |> 
    mutate(age_t = retransform(age, data$age),
           lang_prop_t = retransform(lang_prop, data$lang_prop),
           type = "predicted")
  
  predicted <- predict(model, 
                       newdata = new_data, 
                       re_formula = ~(1|lang_item_id)) |> 
    as_tibble() |> 
    select(Estimate) |> 
    cbind(new_data)
  
  raw_data <- data |>
    filter(uni_lemma %in% unilemmas) |>
    mutate(Estimate = as.numeric(value) - 1) |>
    select(age, freq, lang_prop, concreteness, mlu, 
           age_t, freq_t, lang_prop_t, concreteness_t, mlu_t, 
           te_known, language, lang_item_id, item_definition, uni_lemma,
           Estimate) |>
    mutate(type = "raw")
  
  bind_rows(raw_data, predicted) |> 
    mutate(freq_t_binned = cut(freq_t, c(-Inf,-0.5,0,0.5,Inf)),
           lang_prop_binned = cut(lang_prop, c(0, 40, 60, 100)),
           type = as.factor(type))
}

mr_demo <- get_ul_preds(mr_complete, mr_model_base, demo_lemmas)
mr_ex <- mr_demo |> filter(type == "predicted")

ggplot(data = mr_ex, mapping = aes(col = uni_lemma)) +
  theme() +
  geom_smooth(formula = y ~ x,
              mapping = aes(x = age, y = Estimate), # , lty = te_known),
              method = "glm", method.args = list(family = "quasibinomial"), 
              se = FALSE) +
  # scale_linetype_discrete(name = "TE knowledge", 
  #                         labels = c("TE not known", "TE known")) +
  scale_color_manual(name = "Unilemma",
                     values = c("#24639e", "#cf4563")) +
  facet_grid(language ~ lang_prop_binned) +
  labs(x = "Age", y = "Value") +
  ylim(0, 1) +
  scale_x_continuous(sec.axis = sec_axis(~ . , 
                                         name = "Language exposure", 
                                         breaks = NULL, 
                                         labels = NULL))
```

Some of the variability between words is due to the contributions of various word-related predictors, which we can visualise by plotting trajectories at different levels of the predictor. 
For example, the effect of frequency on word knowledge is shown in Figure \@ref(fig:mr-pred). which demonstrates that word knowledge increases with age, with language exposure, and with word frequency.

(ref:mr-pred-cap) Effect of word frequency on word production, derived from Model 1 fitted on the English--Spanish dataset from Marchman.
```{r mr-pred, out.width="100%", fig.cap="(ref:mr-pred-cap)"}
mr_complete_pred <- readRDS("outputs/mr_complete_pred.rds")
ggplot() +
  theme() +
  geom_smooth(data = mr_complete_pred,
              formula = y ~ x,
              aes(x = age, y = Estimate, col = freq_t_binned), #, lty = te_known),
              method = "glm", method.args = list(family = "quasibinomial"), se = FALSE) +
  scale_color_discrete_sequential(name = "Std freq", palette = "BluGrn") +
  facet_grid(language ~ lang_prop_binned) +
  labs(x = "Age", y = "Value") +
  ylim(0, 1) +
  scale_x_continuous(sec.axis = sec_axis(~ . , 
                                         name = "Language exposure", 
                                         breaks = NULL, 
                                         labels = NULL))
```

### Model 1
```{r data-base}
mp_b <- readRDS("outputs/mp_b.rds")
```
Figure \@ref(fig:coef-base) shows the coefficient estimates for each predictor in Model 1 across all datasets. 
We find strong effects of most predictors in the expected directions: age ($\bar\beta =$ `r mp_b |> filter(Parameter == "b_age_t") |> pull(Estimate) |> mean()`), frequency ($\bar\beta =$ `r mp_b |> filter(Parameter == "b_freq_t") |> pull(Estimate) |> mean()`), proportion of exposure to the language ($\bar\beta =$ `r mp_b |> filter(Parameter == "b_lang_prop_t") |> pull(Estimate) |> mean()`), concreteness ($\bar\beta =$ `r mp_b |> filter(Parameter == "b_concreteness_t") |> pull(Estimate) |> mean()`), and MLU-w ($\bar\beta =$ `r mp_b |> filter(Parameter == "b_mlu_t") |> pull(Estimate) |> mean()`). 
Interestingly, there is no reliable interaction effect between frequency and proportion exposure ($\bar\beta =$ `r mp_b |> filter(Parameter == "b_log_pf_t") |> pull(Estimate) |> mean()`), as the estimates which lay beyond the region of practical equivalence had Bayes factor of < 2. 

(ref:coef-base-cap) Estimates of predictor coefficients for Model 1 fit on each dataset. Each point represents the coefficient for one dataset, and the bar displays the mean across all datasets. Filled points indicate coefficients with a Bayes factor of > 1000. The shaded area represents the region of practical equivalence to the null.
```{r coef-base, out.width='2.25in', out.height='3in', fig.width=3.37, fig.height=4.5, fig.align='center', fig.cap="(ref:coef-base-cap)"}
plot_coefs(mp_b)
```

### Model 2
```{r data-te}
mp_t <- readRDS("outputs/mp_t.rds")
```

Figure \@ref(fig:coef-te) shows the coefficient estimates for each predictor in Model 2 across all datasets. 
The main effects are largely similar to those of Model 1, so we focus on the effects related to TEs.
First, there is a main effect of TE knowledge ($\bar\beta =$ `r mp_t |> filter(Parameter == "b_te_known1") |> pull(Estimate) |> mean()`), although there is relatively large variability, with two models having posteriors that are >\ 95% within the region of practical equivalence.
There is also a negative TE interaction effect with age ($\bar\beta =$ `r mp_t |> filter(Parameter == "b_age_t:te_known1") |> pull(Estimate) |> mean()`); in other words, TE knowledge provides a greater boost to younger children. 
Frequency, as well as frequency by language exposure, both appeared to have interaction effects with TE, although these seemed to be driven primarily by an outlier. 
All other interaction effects had posterior distributions that largely fell within the region of practical equivalence.

(ref:coef-te-cap) Estimates of predictor coefficients (main effects and interactions with TE) for Model 2 fit on each dataset. Each point represents the coefficient for one dataset, and the bar displays the mean across all datasets. Filled points indicate coefficients with a Bayes factor of > 1000. The shaded area represents the region of practical equivalence to the null.
```{r coef-te, out.width='3.75in', out.height='3in', fig.width=5.625, fig.height=4.5, fig.align='center', fig.cap="(ref:coef-te-cap)"}
plot_coefs(mp_t)
```

### Model 3
```{r data-full}
mp_f <- readRDS("outputs/mp_f.rds")
```

Figure \@ref(fig:coef-full) shows the coefficient estimates for each predictor in Model 3 across all datasets. 
The main and TE interaction effects are largely similar to those of Model 2.
There is also an additional interaction effect between TE knowledge and phonological similarity  ($\bar\beta =$ `r mp_f |> filter(Parameter == "b_te_known1:overlap_t") |> pull(Estimate) |> mean()`), such that TE knowledge provides a greater boost when the TEs have greater phonological similarity.
All other interaction effects had posterior distributions that largely fell within the region of practical equivalence.

(ref:coef-full-cap) Estimates of predictor coefficients (main effects, and interactions with phonological similarity and/or TE) for Model 3 fit on each dataset. Each point represents the coefficient for one dataset, and the bar displays the mean across all datasets. Filled points indicate coefficients with a Bayes factor of > 1000. The shaded area represents the region of practical equivalence to the null.
```{r coef-full, out.width='6in', out.height='3in', fig.width=9, fig.height=4.5, fig.align='center', fig.cap="(ref:coef-full-cap)"}
plot_coefs(mp_f)
```

```{r cor-full-data}
mp_f_mat <- mp_f |> select(Parameter, MAP, Dataset) |> 
  pivot_wider(names_from = Dataset, values_from = MAP) |> 
  column_to_rownames(var = "Parameter") |> 
  as.matrix() |> 
  cor()
```

#### Consistency.
We additionally investigated the consistency in Model 3 predictor magnitude across the datasets to determine the extent to which different language combinations and data origins share similar factors affecting word knowledge. 
The correlogram is shown in Figure \@ref(fig:cor-full).
The mean pairwise correlation is `r (sum(mp_f_mat) - 4) / 12`, comparable to the production means found by Braginsky et al. (2019) for production data. 

(ref:cor-full-cap) Correlogram of predictor magnitudes for Model 3 across all datasets.
```{r cor-full, out.width="100%", fig.cap="(ref:cor-full-cap)"}
corrplot(mp_f_mat, 
         method = "color", 
         addCoef.col = 'white', 
         tl.col = "black", 
         tl.srt = 45, 
         tl.cex = .8, 
         tl.offset = 1,
         cl.ratio = .25, 
         number.cex = .8, 
         mar = rep(1,4),
         family = .font)
```

#### Lexical categories
```{r data-llc}
mp_lc_f <- readRDS("outputs/mp_lc_f.rds")
```

Previous research has suggested that different lexical categories may be sensitive to different predictors [@braginskyConsistencyVariabilityChildren2019; @goodmanDoesFrequencyCount2008]. 
As such, we subdivided each dataset into lexical categories (nouns, verbs, adjectives, function words, and others) and reran Model 3 on each subset,[^04-03] dropping the "others" category due to its internal heterogeneity.

[^04-03]: This approach had better interpretability relative to the alternative, which would be to include lexical category as a moderator variable, thereby greatly increasing the number of interaction terms.

Figure \@ref(fig:coef-lc) shows the coefficient estimates for each predictor in Model 3 across all datasets, subsetted by lexical category. 
These results confirm previous findings that frequency is a more important predictor for nouns ($\bar\beta =$ `r mp_lc_f |> filter(Parameter == "b_freq_t", lexical_class == "nouns") |> pull(Estimate) |> mean()`) than for function words ($\bar\beta =$ `r mp_lc_f |> filter(Parameter == "b_freq_t", lexical_class == "function_words") |> pull(Estimate) |> mean()`). 
Broadly, most of the other predictors seem to be relatively consistent among lexical categories, with two notable exceptions.
First, the age-by-TE interaction is stronger for verbs ($\bar\beta =$ `r mp_lc_f |> filter(Parameter == "b_age_t:te_known1", lexical_class == "verbs") |> pull(Estimate) |> mean()`) than for other lexical categories, suggesting a greater TE boost for verbs in younger children.
Second, the frequency-by-TE interaction is positive for adjectives ($\bar\beta =$ `r mp_lc_f |> filter(Parameter == "b_freq_t:te_known1", lexical_class == "adjectives") |> pull(Estimate) |> mean()`) whereas it is negative for other lexical categories; this may be driven by particular semantic categories (e.g., colour words), which both have higher frequency and are commonly learnt by children in both languages.

(ref:coef-lc-cap) Estimates of predictor coefficients (main effects, and interactions with phonological similarity and/or TE) for Model 3 fit on each dataset, subsetted by lexical category. Each point represents the coefficient for one dataset, and the bar displays the mean across all datasets. Filled points indicate coefficients with a Bayes factor of > 1000. The shaded area represents the region of practical equivalence to the null.
```{r coef-lc, out.width='6in', out.height='3.2in', fig.width=9, fig.height=4.8, fig.align='center', fig.cap="(ref:coef-lc-cap)"}
ggplot(mp_lc_f, aes(x = Estimate, 
                    y = fct_rev(fct_inorder(Predictor)),
                    col = lexical_class)) +
  facet_grid(. ~ Effect, scales = "free") +
  geom_rect(aes(xmin = ROPE_low[1], xmax = ROPE_high[1],
                  ymin = -Inf, ymax = Inf),
              fill = "gray90", alpha = .1, color = NA) +
  geom_vline(xintercept = 0, color = "gray30", linetype = "dashed") +
  geom_point(aes(shape = log_BF <= 3), size = 1, alpha = 1) +
  ggstance::stat_summaryh(geom = "crossbarh", 
                          fun.x = mean, fun.xmin = mean, fun.xmax = mean,
                          fatten = 1) +
  scale_colour_discrete(name = "Lexical category",
                        labels = c("Adjectives", "Function words", "Nouns", "Verbs")) +
  scale_shape_manual(values = c(19, 1), guide = "none") +
  labs(x = "Coefficient estimate", y = "") +
  theme(legend.position = "bottom")
```

## Discussion
The results of this study reflect that (1) TEs serve an important role in bilingual word learning by presenting an additional source of language information, and that (2) the standard model remains a useful modelling framework in bilingual contexts. 
We defer discussion of the first observation to the General Discussion, and examine the second below.

The current analyses reveal that many of the factors affecting word learning in the monolingual context [see @braginskyConsistencyVariabilityChildren2019] also have a similar effect in the bilingual context, including age, frequency, concreteness, and MLU-w. 
Furthermore, they confirm that the standard model is largely a meaningful and effective model of language learning in the bilingual context, with input-related and word difficulty-related variability being two important dimensions affecting word learning. 
Specifically, children who are older (i.e., who have received more language input) are more likely to know words, and words that are more frequent (i.e., that occupy a larger proportion of language input) are more likely to be known.
Additionally, words that are more abstract and that occur in more syntactically complex sentences are more difficult and thus learned later. 
These findings are reinforced by the result that the effects of such variability are relatively consistent across datasets, suggesting that acquisition strategies are largely similar across these populations despite cultural and linguistic differences. 
In other words, the standard model is applicable across cultures and languages as an effective model of word learning. 

One interesting point of deviation from the standard model is the lack of an interaction effect between frequency and proportion of language exposure. 
Under the standard model, we can model the proportion of a word's occurrence in a child's language input as the product of frequency and proportion of language exposure, and would thus expect an interaction effect to emerge, but no such effect was reliably observed. 
One possible reason is that the non-effect was due to the way the model was specified: The model included proportion of language exposure without a log-transform, whereas frequency and the interaction of frequency and language exposure were both log-transformed.
This resulted in some multicollinearity between the latter two, and as such, the variance may have been split between the two predictors. 
Another explanation may be that input-related factors are titrated by total amount of linguistic input received by the child (i.e., the important metric is _expected number of tokens_ of a word), and the absence of such data may result in insufficient sensitivity to detect an effect [see @marchmanCaregiverTalkYoung2017]. 
These explanations are not mutually exclusive, and further investigation into the relationship between language input and word learning is imperative to better determine the validity of the present result; for example, studies that use dense recordings to give more exact, absolute metrics on a child's language input may be more sensitive to smaller effects of input-related factors.

Methodologically, this study also adds support to the utility of using by-child and by-item analyses on large datasets. 
While perhaps more computationally intensive than models which collapse vocabulary data along either of these dimensions, this allows for the inclusion of crucial variables which vary across children (e.g. language exposure) and across items within children (e.g. TE knowledge). 
The large number of datapoints also means that models are more sensitive to the effect of various predictors, and permits a large multifactorial analysis incorporating many variables of interest. 
Such an approach can thus help to elucidate the contribution of such variables while accounting for other covariates, thereby providing better estimates of true effect sizes. 
We encourage further exploration using item-level data to better understand the processes and patterns underlying bilingual word learning. 
